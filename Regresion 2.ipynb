{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3f2580-089a-4672-b090-8ae4d452d001",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1832de55-0d08-4650-adc3-366548f9ef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#R-squared is a statistical measure of how well the regression line fits the data.\n",
    "#It is calculated as the percentage of the variance in the dependent variable that is explained by the independent variable.\n",
    "3A higher R-squared value indicates a better fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fd7903-ee4b-448d-8dab-047f5e55c9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179ae6dd-b873-4431-9e17-25f663ee3e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjusted R-squared is a statistical measure used to assess the goodness of fit of a regression model. \n",
    "#It differs from the regular R-squared by taking into account the number of predictors in the model, penalizing the inclusion of irrelevant variables.\n",
    "#It adjusts the R-squared value to provide a more accurate representation of a model's explanatory power while accounting for model complexity.\n",
    "# Adjusted R square is always less than r Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7dc7a5-7bae-4775-af82-adf8d38278dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7329b4de-6520-4708-91a4-16413cf12f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjusted R-squared is more appropriate to use when you have a model with multiple independent variables.\n",
    "#This is because R-squared can be inflated by the number of independent variables in the model, \n",
    "#while adjusted R-squared takes into account the number of independent variables and penalizes the R-squared value for having too many variables.\n",
    "#Therefore, adjusted R-squared is a better measure of how well a model fits the data when there are multiple independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ab194f-e4e1-44a2-912d-dc69f2120876",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48f3921-12f9-4a7b-b2c4-788a9e533e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''RMSE (Root Mean Square Error): RMSE is a commonly used regression metric that quantifies the average magnitude \n",
    "of the errors between predicted and actual values. It is calculated by taking the square root of the mean of the\n",
    "squared differences between predictions and actual values. RMSE gives more weight to larger errors and is sensitive to outliers.\n",
    "\n",
    "MSE (Mean Squared Error): MSE is another measure of the average error in regression analysis. It is calculated by\n",
    "averaging the squared differences between predicted and actual values. MSE also emphasizes larger errors but lacks\n",
    "interpretability as it is not on the same scale as the original data.\n",
    "\n",
    "MAE (Mean Absolute Error): MAE represents the average absolute differences between predicted and actual values.\n",
    "It is calculated by taking the mean of the absolute values of the errors. MAE is more robust to outliers and provides a more intuitive understanding of the average prediction error.\n",
    "\n",
    "In summary, RMSE, MSE, and MAE are metrics used to evaluate the accuracy of regression models.\n",
    "RMSE and MSE emphasize larger errors, while MAE gives equal weight to all errors. Lower values of these metrics indicate better model performance,\n",
    "with RMSE and MSE providing a more penalizing view of larger errors compared to MAE.'''\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b5e1e9-8704-4f82-8b0b-151397e5d2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118e70ba-974d-4151-9fce-185145f43ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Advantages:\n",
    "\n",
    "#RMSE: Emphasizes larger errors, which can be useful when small errors are less significant. It penalizes outliers, making it suitable when outliers need to be addressed.\n",
    "#MSE: Similar to RMSE but without the square root operation, providing a computationally simpler measure. It is useful for mathematical optimization and gradient-based techniques.\n",
    "\n",
    "#Disadvantages:\n",
    "\n",
    "#RMSE and MSE: Both can be sensitive to outliers, giving undue importance to extreme errors.\n",
    "#MAE: While robust to outliers, MAE may not differentiate well between models with different error distributions, as it treats all errors equally. \n",
    "# It can be less informative when understanding the significance of prediction errors in certain applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7520ab-7f70-417d-bcbf-1bbce87f43c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949d2a77-489b-4a7a-8950-6f83a4c77f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Lasso regularization is a technique used in linear regression and machine learning to prevent overfitting by \n",
    "adding a penalty term to the regression equation, which encourages the model to select a subset of the most important predictors while setting others to zero. \n",
    "This leads to feature selection and a more interpretable model.\n",
    "\n",
    "Lasso differs from Ridge regularization in that it can result in feature selection by setting some coefficients to exactly zero, \n",
    "while Ridge only shrinks coefficients towards zero but doesn't eliminate them entirely. Lasso is more appropriate when you suspect\n",
    "that many of your input features are irrelevant or redundant, and you want a simpler, more interpretable model with fewer predictors.\n",
    "Ridge may be preferred when all features are potentially relevant,\n",
    "and you aim to reduce multicollinearity and control coefficients' magnitudes without excluding any variables.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e489cbdf-3e97-4a94-917d-1945fbdda9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e60e10ff-62f0-4a09-adb4-d7b4073dcb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regularized linear models prevent overfitting by adding a penalty term to the loss function that discourages the model from fitting the training data too closely. \n",
    "#This penalty constrains the model's coefficients, preventing them from becoming overly large. This helps generalize better to unseen data.\n",
    "\n",
    "#For example, in Ridge regression, the L2 regularization term adds the sum of squared coefficients to the loss function. \n",
    "#If a feature is irrelevant or has noise, Ridge will shrink its coefficient towards zero, reducing its influence on the model,\n",
    "#effectively preventing overfitting by dampening the impact of less important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934eff06-7f5f-4611-bc75-694307ac84b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfa6b49-cfa3-44f8-ba88-980702ef19ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regularized linear models may not be the best choice when dealing with highly nonlinear data, as they are inherently linear. \n",
    "#They can also struggle with datasets where feature engineering is challenging or when complex interactions between variables are critical.\n",
    "#Additionally, in cases where model interpretability is a priority, simpler models without regularization may be preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13982bc3-68df-4361-8818-aed4c3a4fd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q9. You are comparing the performance of two regression models using different evaluation metrics.Model A has an RMSE of 10, while Model B has an MAE of 8. \n",
    "#Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c132a1bc-c41a-4657-a341-323133d5f132",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this case, Model B has a lower MAE than Model A. \n",
    "#This means that Model B has a smaller average error between the predicted values and the actual values. Therefore, I would choose Model B as the better performer.\n",
    "\n",
    "#Limitations\n",
    "#It is important to note that MAE is more sensitive to outliers than RMSE. This means that if there are a few outliers in the data, MAE will be more affected than RMSE.\n",
    "\n",
    "#In this case, if the outliers are large, then Model A may actually be the better performer. This is because RMSE is less affected by outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3270edb7-39b2-4c48-b050-bd053839d1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a \n",
    "#regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "#better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c49c24-6d24-4f88-9b5d-5d2e34e7bc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ridge regularization with a regularization parameter of 0.1 will shrink the coefficients of the model, but it will not force any of the coefficients to be equal to zero.\n",
    "This means that Model A may still have some redundant features.\n",
    "Lasso regularization with a regularization parameter of 0.5 will shrink the coefficients of the model and may force some of the coefficients to be equal to zero. \n",
    "This means that Model B may select a subset of the features that are most important for the model.\n",
    "\n",
    "In general, Lasso regularization is more effective at feature selection than Ridge regularization. However, Lasso regularization can also be more prone to overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
